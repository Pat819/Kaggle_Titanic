{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"C:/Users/kpleu/Desktop/Git/Kaggle_Titanic/data/train.csv\")\n",
    "predict_set=pd.read_csv(\"C:/Users/kpleu/Desktop/Git/Kaggle_Titanic/data/test.csv\")\n",
    "\n",
    "# Combine Dataset for cleaning\n",
    "dataset_cleaning=[train,predict_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and preprocessing:\n",
    "\n",
    "1. Check for NaN values and decide the treatment e.g. removing the entries or imputing with meaningful values\n",
    "2. Convert categorical data into numerical e.g. onehotencoder\n",
    "3. Look for any abnormal data via a scatterplot matrix which also provide a very first impression of how the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(train.drop(['PassengerId', 'Name','Ticket'], axis=1), hue=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex and Embarked mapping\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_Sex(value):\n",
    "    # Return 0 if value is 'female'\n",
    "    if value == 'female':\n",
    "        return 0   \n",
    "    # Return 1 if value is 'male'    \n",
    "    elif value == 'male':\n",
    "        return 1    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Define recode_Embarked()\n",
    "def recode_Embarked(value):\n",
    "    # Return 0 if value is 'C = Cherbourg'\n",
    "    if value == 'C':\n",
    "        return 'Cherbourg'   \n",
    "    # Return 1 if value is 'Q = Queenstown'    \n",
    "    elif value == 'Q':\n",
    "        return 'Queenstown'\n",
    "    # Return 2 if value is 'S = Southampton'    \n",
    "    elif value == 'S':\n",
    "        return 'Southampton'  \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "for dataset in dataset_cleaning:\n",
    "    dataset['Sex']=dataset['Sex'].apply(recode_Sex)\n",
    "    dataset['Embarked']=dataset['Embarked'].apply(recode_Embarked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that contain NaN values include: Age, Cabin, Embarked\n",
    "\n",
    "For Cabin, NaN should be replaced by 0 to indicate that the passenger was travelling without a cabin\n",
    "\n",
    "For Age, NaN value can be replaced by the mean value of the sub group based on sex and title (To be completed after further investigation in the dataset)\n",
    "\n",
    "For Embarked, since the observations that are related to the 2 NaN values have most of the features repeated except for Age and Name, it is believed that there might be errors involved when entering the data. As such, removing the two particular entries should be acceptable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_cleaning:\n",
    "    dataset['With_Cabin']=dataset['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "train=train.dropna(subset=['Embarked'])\n",
    "predict_set=predict_set.dropna(subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, pd.get_dummies(train['Embarked'],drop_first=True,prefix='Embarked')], axis=1)\n",
    "predict_set = pd.concat([predict_set, pd.get_dummies(predict_set['Embarked'],drop_first=True,prefix='Embarked')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and clean the titles from the passenger's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title']=train['Name'].apply(lambda x: re.search('([A-Za-z]+)\\.', x).group(1))\n",
    "train['Title']=train['Title'].astype('category')\n",
    "\n",
    "predict_set['Title']=predict_set['Name'].apply(lambda x: re.search('([A-Za-z]+)\\.', x).group(1))\n",
    "predict_set['Title']=predict_set['Title'].astype('category')\n",
    "\n",
    "def Frequency_table(data):\n",
    "    frequencytable = {}\n",
    "    for key in data:\n",
    "        if key in frequencytable:\n",
    "            frequencytable[key] += 1\n",
    "        else:\n",
    "            frequencytable[key] = 1\n",
    "    return frequencytable\n",
    "\n",
    "Frequency_table(train['Title'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the column of Title and named it as Title_cleaned\n",
    "train['Title_cleaned']=train['Title']\n",
    "# Converting French title to English title\n",
    "train['Title_cleaned']=train['Title_cleaned'].replace(['Mlle','Ms'],'Miss')\n",
    "train['Title_cleaned']=train['Title_cleaned'].replace(['Mme'],'Mrs')\n",
    "# Group all other title as 'Other'\n",
    "train['Title_cleaned']=train['Title_cleaned'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Rev','Sir'],'Other')\n",
    "    \n",
    "# Duplicate the column of Title and named it as Title_cleaned\n",
    "predict_set['Title_cleaned']=predict_set['Title']\n",
    "# Converting French title to English title\n",
    "predict_set['Title_cleaned']=predict_set['Title_cleaned'].replace(['Mlle','Ms'],'Miss')\n",
    "predict_set['Title_cleaned']=predict_set['Title_cleaned'].replace(['Mme'],'Mrs')\n",
    "# Group all other title as 'Other'\n",
    "predict_set['Title_cleaned']=predict_set['Title_cleaned'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Rev','Sir'],'Other')\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train['Title_cleaned'],drop_first=True,prefix='Title')], axis=1)\n",
    "predict_set = pd.concat([predict_set, pd.get_dummies(predict_set['Title_cleaned'],drop_first=True,prefix='Title')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaN of age with the mean age according to title group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean_0=train[train['Title_cleaned']=='Master']['Age'].mean()\n",
    "age_mean_1=train[train['Title_cleaned']=='Miss']['Age'].mean()\n",
    "age_mean_2=train[train['Title_cleaned']=='Mr']['Age'].mean()\n",
    "age_mean_3=train[train['Title_cleaned']=='Mrs']['Age'].mean()\n",
    "age_mean_4=train[train['Title_cleaned']=='Other']['Age'].mean()\n",
    "\n",
    "age_mean_list=[age_mean_0,age_mean_1,age_mean_2,age_mean_3,age_mean_4]\n",
    "\n",
    "\n",
    "for row in range(0,len(train.index)):\n",
    "    if math.isnan(train.iloc[row]['Age']):\n",
    "        if train.loc[row,'Title_cleaned'] == 'Master':\n",
    "             train.iat[row,5]=age_mean_list[0]\n",
    "        elif train.loc[row,'Title_cleaned'] == 'Miss':\n",
    "             train.iat[row,5]=age_mean_list[1]\n",
    "        elif train.loc[row,'Title_cleaned'] == 'Mr':\n",
    "             train.iat[row,5]=age_mean_list[2]\n",
    "        elif train.loc[row,'Title_cleaned'] == 'Mrs':\n",
    "             train.iat[row,5]=age_mean_list[3]\n",
    "        elif train.loc[row,'Title_cleaned'] == 'Other':\n",
    "             train.iat[row,5]=age_mean_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(0,len(predict_set.index)):\n",
    "    if math.isnan(predict_set.iloc[row]['Age']):\n",
    "        if predict_set.loc[row,'Title_cleaned'] == 'Master':\n",
    "            predict_set.iat[row,4]=age_mean_list[0]\n",
    "        elif predict_set.loc[row,'Title_cleaned'] == 'Miss':\n",
    "            predict_set.iat[row,4]=age_mean_list[1]\n",
    "        elif predict_set.loc[row,'Title_cleaned'] == 'Mr':\n",
    "            predict_set.iat[row,4]=age_mean_list[2]\n",
    "        elif predict_set.loc[row,'Title_cleaned'] == 'Mrs':\n",
    "            predict_set.iat[row,4]=age_mean_list[3]\n",
    "        elif predict_set.loc[row,'Title_cleaned'] == 'Other':\n",
    "            predict_set.iat[row,4]=age_mean_list[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicated entries, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop_duplicates()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NAN values in predict_set['Fare'] with the average\n",
    "predict_set['Fare'].fillna(predict_set['Fare'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select features to be included in the logistic model\n",
    "\n",
    "feature=['Pclass','Sex','Age','SibSp','Parch','Fare','With_Cabin','Embarked_Queenstown','Embarked_Southampton','Title_Miss','Title_Mr','Title_Mrs','Title_Other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[feature],train['Survived'],test_size=0.3,random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg,param_grid,cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction for sumbission \n",
    "predict_set['Survived']=logreg_cv.predict(predict_set[feature])\n",
    "predict_set[['PassengerId', 'Survived']].to_csv('C:/Users/kpleu/Desktop/Git/Kaggle_Titanic/Submission/logreg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
